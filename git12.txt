finds 

import pandas as pd
import numpy as np
 
#to read the data in the csv file
data = pd.read_csv("F:/finders.csv")
print(data,"n")
 
#making an array of all the attributes
d = np.array(data)[:,:-1]
print("n The attributes are: ",d)
 
#segragating the target that has positive and negative examples
target = np.array(data)[:,-1]
print("n The target is: ",target)
 
#training function to implement find-s algorithm
def train(c,t):
    for i, val in enumerate(t):
        if val == "POSITIVE":
            specific_hypothesis = c[i].copy()
            break
             
    for i, val in enumerate(c):
        if t[i] == "POSITIVE":
            for x in range(len(specific_hypothesis)):
                if val[x] != specific_hypothesis[x]:
                    specific_hypothesis[x] = '?'
                else:
                    pass
                 
    return specific_hypothesis
 
#obtaining the final hypothesis
print("n The final hypothesis is:",train(d,target))







candidate algorithum

import numpy as np 
import pandas as pd

data = pd.read_csv('F:\MY ACCADEMICS  V SEM\ML\DATASETS\enjoy_sports.csv')
concepts = np.array(data)[:,0:-1]
target = np.array(data)[:,-1]

def learn(concepts, target): 
    specific_h = concepts[0].copy()
    general_h = [["?" for i in range(len(specific_h))] for i in range(len(specific_h))]

    for i, h in enumerate(concepts):
        if target[i] == "Yes":
            for x in range(len(specific_h)): 
                if h[x]!= specific_h[x]:                    
                    specific_h[x] ='?'                     
                    general_h[x][x] ='?'
                   
        if target[i] == "No":            
            for x in range(len(specific_h)): 
                if h[x]!= specific_h[x]:                    
                    general_h[x][x] = specific_h[x]                
                else:                    
                    general_h[x][x] = '?'        
        
  
    indices = [i for i, val in enumerate(general_h) if val == ['?', '?', '?', '?', '?', '?']]    
    for i in indices:   
        general_h.remove(['?', '?', '?', '?', '?', '?']) 
    return specific_h, general_h 

s_final, g_final = learn(concepts, target)

print("Final Specific_h: ", s_final, sep="\n")
print("Final General_h: ", g_final, sep="\n")









linear regression

# importing the dataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
  
dataset = pd.read_csv('F:/MY ACCADEMICS  V SEM/ML/DATASETS/Salary_Data.csv')
dataset.head()
 
# data preprocessing
X = np.array(dataset)[:,:-1]  #independent variable array
y = np.array(dataset)[:,-1]  #dependent variable vector
 
# splitting the dataset
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=1/3,random_state=0)
 
# fitting the regression model
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train,y_train) #actually produces the linear eqn for the data
 
# predicting the test set results
y_pred = regressor.predict(X_test) 
y_pred
 
y_test
 
# visualizing the results
#plot for the TRAIN
  
plt.scatter(X_train, y_train, color='red') # plotting the observation line
plt.plot(X_train, regressor.predict(X_train), color='blue') # plotting the regression line
plt.title("Salary vs Experience (Training set)") # stating the title of the graph
  
plt.xlabel("Years of experience") # adding the name of x-axis
plt.ylabel("Salaries") # adding the name of y-axis
plt.show() # specifies end of graph
 
#plot for the TEST
  
plt.scatter(X_test, y_test, color='red') 
plt.plot(X_test, regressor.predict(X_test), color='blue') # plotting the regression line
plt.title("Salary vs Experience (Testing set)")
  
plt.xlabel("Years of experience") 
plt.ylabel("Salaries") 
plt.show() 
from sklearn.metrics import r2_score
score = r2_score(y_test,y_pred)
print(score)

for i in range(len(y_test)):
    print("actual value",y_test[i],"predicted value",y_pred[i])
    

pred_y_df = pd.DataFrame({'actual Value   ':y_test,'Predicted Value      ':y_pred,'diffrence':y_pred-y_test})
pred_y_df








multiple linear regression

import numpy as np
import pandas as pd
data = pd.read_csv("F:/MY ACCADEMICS  V SEM/ML/DATASETS/powerstation.csv")
data.head()
x = data.drop(['PE'],axis=1).values
y = data['PE'].values
print(y)
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=0)
from sklearn.linear_model import LinearRegression

ml = LinearRegression()
ml.fit(x_train,y_train)
y_pred = ml.predict(x_test)
print(y_pred)
print(y_test)
ml.predict([[8.34,40.77,1010.84,90.01]])
from sklearn.metrics import r2_score
r2_score(y_test,y_pred)
import matplotlib.pyplot as plt
plt.figure(figsize=(15,10))
plt.scatter(y_test,y_pred)
plt.xlabel('actual')
plt.ylabel('predicted')
plt.title('actual vs predicted')
pred_y_df = pd.DataFrame({'actual Value':y_test,'Predicted Value':y_pred,'diffrence':y_pred-y_test})
pred_y_df








naive_bayes 

# load the iris dataset
from sklearn.datasets import load_iris
iris = load_iris()
 
# store the feature matrix (X) and response vector (y)
X = iris.data
y = iris.target
 
# splitting X and y into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)
 
# training the model on training set
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(X_train, y_train)
 
# making predictions on the testing set
y_pred = gnb.predict(X_test)
 
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print("Gaussian Naive Bayes model accuracy(in %):", metrics.accuracy_score(y_test, y_pred)*100)








decission tree

#Importing required libraries
import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

#Loading the iris data
data = load_iris()
print('Classes to predict: ', data.target_names)

#Extracting data attributes
X = data.data
### Extracting target/ class labels
y = data.target

print('Number of examples in the data:', X.shape[0])

#Using the train_test_split to create train and test sets.
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 47, test_size = 0.25)

#Importing the Decision tree classifier from the sklearn library.
from sklearn.tree import DecisionTreeClassifier
t = DecisionTreeClassifier()

t.fit(X_train, y_train)

y_pred =  t.predict(X_test)

#Importing the accuracy metric from sklearn.metrics library

from sklearn.metrics import accuracy_score
print('Accuracy Score on train data: ', t.score(X_train,y_train))
print('Accuracy Score on test data: ', t.score(X_test,y_test))








k means

import numpy as nm    
import matplotlib.pyplot as  plt 
import pandas as pd    
from sklearn.cluster import KMeans 
import numpy as np

dataset = pd.read_csv('F:/MY ACCADEMICS  V SEM/ML/assignments/Mall_Customers.csv')  
x = np.array(dataset)[:, [3, 4]]

 
wcss_list= [] 

for i in range(1, 11):  
    kmeans = KMeans(n_clusters=i, init='k-means++')  
    kmeans.fit(x)  
    wcss_list.append(kmeans.inertia_)  
    
plt.plot(range(1, 11), wcss_list)  
plt.title('The Elobw Method Graph')  
plt.xlabel('Number of clusters(k)')  
plt.ylabel('wcss_list')  
plt.show() 

kmeans = KMeans(n_clusters=5, init='k-means++')  
y_predict= kmeans.fit_predict(x) 


plt.scatter(x[y_predict == 0, 0], x[y_predict == 0, 1], s = 100, c = 'blue', label = 'Cluster 1')
plt.scatter(x[y_predict == 1, 0], x[y_predict == 1, 1], s = 100, c = 'green', label = 'Cluster 2')  
plt.scatter(x[y_predict == 2, 0], x[y_predict == 2, 1], s = 100, c = 'red', label = 'Cluster 3')  
plt.scatter(x[y_predict == 3, 0], x[y_predict == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')
plt.scatter(x[y_predict == 4, 0], x[y_predict == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5') 
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroid')   
plt.title('Clusters of customers')  
plt.xlabel('Annual Income (k$)')  
plt.ylabel('Spending Score (1-100)')  
plt.legend()  
plt.show()








knn

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier 
from sklearn import datasets 

iris=datasets.load_iris() 
print("Iris Data set loaded...") 
x_train,x_test,y_train,y_test = train_test_split(iris.data,iris.target,test_size=0.1) 
print("Dataset is split into training and testing samples...") 
print("Size of trainng data and its label",x_train.shape,y_train.shape) 
print("Size of trainng data and its label",x_test.shape, y_test.shape) 

for i in range(len(iris.target_names)): 
    print("Label", i , "-",str(iris.target_names[i])) 
    
classifier = KNeighborsClassifier() 
classifier.fit(x_train,y_train)
y_pred = classifier.predict(x_test) 
print("Results of Classification using K-nn with K=1 ") 


for r in range(len(x_test)): 
    print(" Sample:", str(x_test[r]), " Actual-label:", str(y_test[r]), " Predicted-label:", str(y_pred[r])) 
print("Classification Accuracy :" , classifier.score(x_test,y_test)); 

